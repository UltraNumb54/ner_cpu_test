# preprocessing.py (обновленный для pymorphy3 и корректной фильтрации по классам)

import xml.etree.ElementTree as ET_std
import lxml.etree as ET_lxml
import re
from collections import Counter
import unicodedata
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd  # Добавлен для сохранения в CSV
import pymorphy3  # Импорт pymorphy3 вместо pymorphy2


def load_and_preprocess_xml(xml_file_path, min_class_samples=5, min_words_per_text=3):
    """
    Загружает, фильтрует и предобрабатывает XML.
    min_words_per_text: Минимальное количество слов в тексте для включения в датасет.
    """
    records = []
    parser = ET_lxml.XMLParser(recover=True, encoding='utf-8')
    tree = ET_lxml.parse(xml_file_path, parser)
    root = tree.getroot()

    for record in root.iter('record'):
        rec_id_elem = record.find('id')
        service_elem = record.find('service')
        topic_elem = record.find('topic')
        description_elem = record.find('description')
        attachments_elem = record.find('attachments')

        rec_id = rec_id_elem.text if rec_id_elem is not None and rec_id_elem.text is not None else ""
        service = service_elem.text if service_elem is not None and service_elem.text is not None else ""
        topic = topic_elem.text if topic_elem is not None and topic_elem.text is not None else ""
        description = description_elem.text if description_elem is not None and description_elem.text is not None else ""
        attachments = attachments_elem.text if attachments_elem is not None and attachments_elem.text is not None else ""

        combined_text = f"{topic} {description}".strip()
        records.append({
            'id': rec_id,
            'service': service,
            'raw_text': combined_text,
            'attachments': attachments
        })

    service_counts = Counter([r['service'] for r in records])
    valid_services = {service for service, count in service_counts.items() if count >= min_class_samples}
    filtered_records_by_class = [r for r in records if r['service'] in valid_services]

    processed_records = []
    morph = pymorphy3.MorphAnalyzer()  # Инициализация анализатора pymorphy3
    for record in filtered_records_by_class:
        processed_text = preprocess_text(record['raw_text'], morph)  # Передача morph в функцию
        if processed_text:
            # Подсчет слов
            words = processed_text.split()
            if len(words) >= min_words_per_text:
                processed_records.append({
                    'id': record['id'],
                    'service': record['service'],
                    'combined_text': processed_text,
                    'attachments': record['attachments']
                })

    # --- Новое: Ограничение количества записей по классам ---
    # Группировка записей по классу
    records_by_service = {}
    for record in processed_records:
        service = record['service']
        if service not in records_by_service:
            records_by_service[service] = []
        records_by_service[service].append(record)

    # Фильтрация: оставить только последние 10000 записей в каждом классе, если их больше
    final_records = []
    kept_counts = {}
    for service, service_records in records_by_service.items():
        if len(service_records) > 10000:
            # Берем последние 10000 записей
            kept_records = service_records[-10000:]
            kept_counts[service] = 10000
        else:
            kept_records = service_records
            kept_counts[service] = len(service_records)
        final_records.extend(kept_records)

    # Вывод информации о фильтрации по классам
    print("--- Информация о фильтрации по классам ---")
    for service, kept_count in kept_counts.items():
        original_count = service_counts[service]
        if original_count > 10000:
            print(f"Класс '{service}': отфильтровано с {original_count} до {kept_count} записей (оставлены последние 10000).")
        else:
            print(f"Класс '{service}': {kept_count} записей (без фильтрации).")

    print(f"Общий размер датасета после фильтрации по классам: {len(final_records)}")

    return final_records


def preprocess_text(text, morph):
    """
    Предобрабатывает текст: приводит к нижнему регистру, удаляет не-кириллические символы,
    лемматизирует слова с помощью pymorphy3, нормализует пробелы.
    """
    if not text:
        return ""
    # Приводим к нижнему регистру
    text = text.lower()
    # Удаляем всё, кроме кириллических букв и пробелов
    text = re.sub(r'[^а-яё\s]', ' ', text)

    # Лемматизация с использованием pymorphy3
    words = text.split()
    lemmatized_words = []
    for word in words:
        # Получаем нормальную (лемматизированную) форму слова
        parsed_word = morph.parse(word)[0]  # Берем первый вариант разбора
        normal_form = parsed_word.normal_form
        lemmatized_words.append(normal_form)
    processed_text = ' '.join(lemmatized_words)

    # Нормализуем пробелы (для случая, если лемматизация добавила лишние)
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # Убираем лишние пробелы в начале и конце
    processed_text = processed_text.strip()
    return processed_text


if __name__ == "__main__":
    XML_PATH = "path/to/your/dataset.xml"  # Укажите путь к вашему XML файлу
    MIN_SAMPLES_PER_CLASS = 10
    MIN_WORDS_PER_TEXT = 3

    print("Загрузка и предобработка данных...")
    processed_data = load_and_preprocess_xml(
        XML_PATH,
        min_class_samples=MIN_SAMPLES_PER_CLASS,
        min_words_per_text=MIN_WORDS_PER_TEXT,
    )

    print(f"Загружено {len(processed_data)} записей после фильтрации и предобработки.")
    if processed_data:
        print("Пример предобработанной записи:")
        print(processed_data[0])

        print("\n--- Статистика по датасету ---")
        num_classes = len(set(r['service'] for r in processed_data))
        print(f"Количество уникальных классов (после фильтрации): {num_classes}")

        service_counts_after_filter = Counter([r['service'] for r in processed_data])
        counts = list(service_counts_after_filter.values())

        print(f"Минимальное количество образцов в классе: {min(counts)}")
        print(f"Максимальное количество образцов в классе: {max(counts)}")
        print(f"Среднее количество образцов в классе: {np.mean(counts):.2f}")
        print(f"Медианное количество образцов в классе: {np.median(counts):.2f}")

        text_lengths = [len(r['combined_text']) for r in processed_data]
        print(f"Минимальная длина текста: {min(text_lengths)}")
        print(f"Максимальная длина текста: {max(text_lengths)}")
        print(f"Средняя длина текста: {np.mean(text_lengths):.2f}")

        word_counts = [len(r['combined_text'].split()) for r in processed_data]
        print(f"Минимальное количество слов в тексте: {min(word_counts)}")
        print(f"Максимальное количество слов в тексте: {max(word_counts)}")
        print(f"Среднее количество слов в тексте: {np.mean(word_counts):.2f}")

        # --- Визуализация ---
        fig, ax = plt.subplots(figsize=(14, 8))
        services = list(service_counts_after_filter.keys())
        counts_list = list(service_counts_after_filter.values())
        ax.bar(range(len(services)), counts_list)
        ax.set_title('Распределение количества образцов по классам (после фильтрации)')
        ax.set_xlabel('Класс (service)')
        ax.set_ylabel('Количество образцов')
        ax.set_xticks(range(len(services)))
        ax.set_xticklabels(services, rotation=90, ha="right")  # Подписи под столбцами
        plt.tight_layout()
        plt.show()

        # --- Гистограмма длин текстов ---
        unique_lengths = set(text_lengths)
        if len(unique_lengths) == 1:
            print(f"Все тексты имеют одинаковую длину: {text_lengths[0]} символов. Гистограмма неинформативна.")
        else:
            num_bins = min(50, len(unique_lengths))
            plt.figure(figsize=(10, 6))
            plt.hist(text_lengths, bins=num_bins, edgecolor='black')
            plt.title('Распределение длин текстов (combined_text)')
            plt.xlabel('Длина текста (символы)')
            plt.ylabel('Количество образцов')
            plt.show()

        # --- Сохранение предобработанного датасета ---
        # Обновляем имя файла, чтобы отразить лемматизацию и фильтрацию
        OUTPUT_CSV_PATH = "processed_dataset_lemmatized_filtered.csv"
        df_to_save = pd.DataFrame(processed_data)
        df_to_save.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')
        print(f"\nПредобработанный датасет (лемматизированный, с фильтрацией по 10000 записей на класс) сохранен в файл: {OUTPUT_CSV_PATH}")
    else:
        print("После предобработки и фильтрации не осталось записей.")
