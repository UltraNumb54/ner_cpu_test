!pip install pymorphy2

import xml.etree.ElementTree as ET_std
import lxml.etree as ET_lxml
import re
from collections import Counter, defaultdict
import unicodedata
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import pymorphy2
import warnings

# Подавляем предупреждения от pymorphy2
warnings.filterwarnings("ignore", category=UserWarning, module='pymorphy2')

# Инициализация морфологического анализатора
morph = pymorphy2.MorphAnalyzer()

# Список стоп-слов (стандартные + кастомные)
# Добавьте свои стоп-слова в этот список
CUSTOM_STOPWORDS = [
    "например", "также", "как", "этот", "свой", "ваш", "наш", "их",
    "быть", "иметь", "сделать", "сказать", "такой", "мочь", "получить",
    "помочь", "проблема", "ситуация", "вопрос", "нужно", "можно",
    "спасибо", "добрый", "день", "вечер", "утро", "здрасьте", "привет",
    "пожалуйста", "извините", "сейчас", "тут", "там", "все", "всем",
    "мне", "меня", "себя", "себе", "себя", "себе", "себя", "себе",
    "себя", "себе", "себя", "себе", "себя", "себе", "себя", "себе",
    "пожалуйста", "пожалуйста", "пожалуйста", "пожалуйста", "пожалуйста",
    "проблема", "проблема", "проблема", "проблема", "проблема", "проблема",
    "ошибка", "ошибка", "ошибка", "ошибка", "ошибка", "ошибка"
]

# Объединяем стандартные русские стоп-слова с кастомными
try:
    from nltk.corpus import stopwords
    nltk_stopwords = stopwords.words('russian')
    ALL_STOPWORDS = set(nltk_stopwords + CUSTOM_STOPWORDS)
except:
    ALL_STOPWORDS = set(CUSTOM_STOPWORDS)

def preprocess_text(text):
    """Улучшенная предобработка текста с лемматизацией и удалением ФИО"""
    if not text:
        return ""
    
    # 1. Удаление ФИО (различные форматы)
    text = re.sub(r'([А-Я][а-я]+ ){2}[А-Я][а-я]+', '', text)  # Иван Иванович Петров
    text = re.sub(r'[А-Я]\.[А-Я]\. [А-Я][а-я]+', '', text)   # И.И. Петров
    text = re.sub(r'[А-Я][а-я]+ [А-Я]\.[А-Я]\.', '', text)    # Петров И.И.
    text = re.sub(r'[А-Я][а-я]+ [А-Я][а-я]+', '', text)       # Иван Петров (если в начале)
    
    # 2. Удаляем всё, кроме кириллических букв и пробелов
    text = re.sub(r'[^а-яёА-ЯЁ\s]', ' ', text)
    
    # 3. Приводим к нижнему регистру
    text = text.lower()
    
    # 4. Нормализуем пробелы
    text = re.sub(r'\s+', ' ', text)
    
    # 5. Разделяем "слитные" слова (только для кириллицы)
    text = re.sub(r'(?<=[а-яё])(?=[А-ЯЁ])|(?<=[А-ЯЁ])(?=[А-ЯЁ][а-яё])', ' ', text)
    
    # 6. Еще раз нормализуем пробелы
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    if not text:
        return ""
    
    # 7. Токенизация и лемматизация
    words = text.split()
    lemmatized_words = []
    
    for word in words:
        if len(word) < 2:  # Пропускаем слишком короткие слова
            continue
            
        parsed = morph.parse(word)[0]
        lemma = parsed.normal_form
        
        # Пропускаем леммы, которые являются стоп-словами
        if lemma in ALL_STOPWORDS:
            continue
            
        lemmatized_words.append(lemma)
    
    # 8. Формируем итоговый текст
    return ' '.join(lemmatized_words)

def load_and_preprocess_xml(xml_file_path, min_class_samples=5, max_class_samples=10000, min_words_per_text=3):
    """
    Загружает, фильтрует и предобрабатывает XML с лемматизацией и ограничением классов.
    
    Args:
        max_class_samples: Максимальное количество записей на класс (0 = без ограничения)
    """
    records = []
    parser = ET_lxml.XMLParser(recover=True, encoding='utf-8')
    tree = ET_lxml.parse(xml_file_path, parser)
    root = tree.getroot()

    # Сбор всех записей
    for record in root.iter('record'):
        rec_id_elem = record.find('id')
        service_elem = record.find('service')
        topic_elem = record.find('topic')
        description_elem = record.find('description')
        attachments_elem = record.find('attachments')

        rec_id = rec_id_elem.text if rec_id_elem is not None and rec_id_elem.text is not None else ""
        service = service_elem.text if service_elem is not None and service_elem.text is not None else ""
        topic = topic_elem.text if topic_elem is not None and topic_elem.text is not None else ""
        description = description_elem.text if description_elem is not None and description_elem.text is not None else ""
        attachments = attachments_elem.text if attachments_elem is not None and attachments_elem.text is not None else ""

        combined_text = f"{topic} {description}".strip()
        records.append({
            'id': rec_id,
            'service': service,
            'raw_text': combined_text,
            'attachments': attachments
        })

    # Фильтрация по минимальному количеству записей на класс
    service_counts = Counter([r['service'] for r in records])
    valid_services = {service for service, count in service_counts.items() 
                     if count >= min_class_samples}
    
    filtered_records_by_class = [r for r in records if r['service'] in valid_services]
    
    # Ограничение максимального количества записей на класс
    if max_class_samples > 0:
        class_records = defaultdict(list)
        for record in filtered_records_by_class:
            class_records[record['service']].append(record)
        
        filtered_records_by_class = []
        for service, recs in class_records.items():
            if len(recs) > max_class_samples:
                # Берем последние записи (предполагается, что порядок в XML - хронологический)
                filtered_records_by_class.extend(recs[-max_class_samples:])
            else:
                filtered_records_by_class.extend(recs)

    # Финальная обработка и фильтрация
    processed_records = []
    for record in filtered_records_by_class:
        processed_text = preprocess_text(record['raw_text'])
        if processed_text:
            words = processed_text.split()
            if len(words) >= min_words_per_text:
                processed_records.append({
                    'id': record['id'],
                    'service': record['service'],
                    'combined_text': processed_text,
                    'attachments': record['attachments']
                })

    return processed_records

if __name__ == "__main__":
    XML_PATH = "path/to/your/dataset.xml"
    MIN_SAMPLES_PER_CLASS = 10
    MAX_SAMPLES_PER_CLASS = 10000  # Ограничение на класс
    MIN_WORDS_PER_TEXT = 3

    print("Загрузка и предобработка данных...")
    processed_data = load_and_preprocess_xml(
        XML_PATH,
        min_class_samples=MIN_SAMPLES_PER_CLASS,
        max_class_samples=MAX_SAMPLES_PER_CLASS,
        min_words_per_text=MIN_WORDS_PER_TEXT
    )

    print(f"Загружено {len(processed_data)} записей после фильтрации и предобработки.")
    if processed_data:
        print("Пример предобработанной записи:")
        print(processed_data[0])

        print("\n--- Статистика по датасету ---")
        num_classes = len(set(r['service'] for r in processed_data))
        print(f"Количество уникальных классов (после фильтрации): {num_classes}")

        service_counts_after_filter = Counter([r['service'] for r in processed_data])
        counts = list(service_counts_after_filter.values())

        print(f"Минимальное количество образцов в классе: {min(counts)}")
        print(f"Максимальное количество образцов в классе: {max(counts)}")
        print(f"Среднее количество образцов в классе: {np.mean(counts):.2f}")
        print(f"Медианное количество образцов в классе: {np.median(counts):.2f}")

        text_lengths = [len(r['combined_text']) for r in processed_data]
        print(f"Минимальная длина текста: {min(text_lengths)}")
        print(f"Максимальная длина текста: {max(text_lengths)}")
        print(f"Средняя длина текста: {np.mean(text_lengths):.2f}")

        word_counts = [len(r['combined_text'].split()) for r in processed_data]
        print(f"Минимальное количество слов в тексте: {min(word_counts)}")
        print(f"Максимальное количество слов в тексте: {max(word_counts)}")
        print(f"Среднее количество слов в тексте: {np.mean(word_counts):.2f}")

        # --- Визуализация ---
        fig, ax = plt.subplots(figsize=(14, 8))
        services = list(service_counts_after_filter.keys())
        counts_list = list(service_counts_after_filter.values())
        ax.bar(range(len(services)), counts_list)
        ax.set_title('Распределение количества образцов по классам (после фильтрации)')
        ax.set_xlabel('Класс (service)')
        ax.set_ylabel('Количество образцов')
        ax.set_xticks(range(len(services)))
        ax.set_xticklabels(services, rotation=90, ha="right")
        plt.tight_layout()
        plt.show()

        # --- Сохранение предобработанного датасета ---
        OUTPUT_CSV_PATH = "processed_dataset.csv"
        df_to_save = pd.DataFrame(processed_data)
        df_to_save.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')
        print(f"\nПредобработанный датасет сохранен в файл: {OUTPUT_CSV_PATH}")
        
