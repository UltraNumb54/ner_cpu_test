import xml.etree.ElementTree as ET_std
import lxml.etree as ET_lxml
import re
from collections import Counter
import unicodedata
import numpy as np
import matplotlib.pyplot as plt

def load_and_preprocess_xml(xml_file_path, min_class_samples=5):
    records = []
    parser = ET_lxml.XMLParser(recover=True, encoding='utf-8')
    tree = ET_lxml.parse(xml_file_path, parser)
    root = tree.getroot()

    for record in root.iter('record'):
        rec_id_elem = record.find('id')
        service_elem = record.find('service')
        topic_elem = record.find('topic')
        description_elem = record.find('description')
        attachments_elem = record.find('attachments')

        rec_id = rec_id_elem.text if rec_id_elem is not None and rec_id_elem.text is not None else ""
        service = service_elem.text if service_elem is not None and service_elem.text is not None else ""
        topic = topic_elem.text if topic_elem is not None and topic_elem.text is not None else ""
        description = description_elem.text if description_elem is not None and description_elem.text is not None else ""
        attachments = attachments_elem.text if attachments_elem is not None and attachments_elem.text is not None else ""

        combined_text = f"{topic} {description}".strip()
        records.append({
            'id': rec_id,
            'service': service,
            'raw_text': combined_text,
            'attachments': attachments
        })

    service_counts = Counter([r['service'] for r in records])
    valid_services = {service for service, count in service_counts.items() if count >= min_class_samples}
    filtered_records = [r for r in records if r['service'] in valid_services]

    processed_records = []
    for record in filtered_records:
        processed_text = preprocess_text(record['raw_text'])
        if processed_text:
            processed_records.append({
                'id': record['id'],
                'service': record['service'],
                'combined_text': processed_text,
                'attachments': record['attachments']
            })

    return processed_records

def preprocess_text(text):
    if not text:
        return ""
    # Удаляем всё, кроме кириллических букв и пробелов
    text = re.sub(r'[^а-яёА-ЯЁ\s]', ' ', text)
    # Нормализуем пробелы
    text = re.sub(r'\s+', ' ', text)
    # Пробуем разделить "слитные" слова (только для кириллицы)
    text = re.sub(r'(?<=[а-яё])(?=[А-ЯЁ])|(?<=[А-ЯЁ])(?=[А-ЯЁ][а-яё])', ' ', text)
    # Нормализуем пробелы после вставки
    text = re.sub(r'\s+', ' ', text)
    # Убираем лишние пробелы в начале и конце
    text = text.strip()
    return text

if __name__ == "__main__":
    XML_PATH = "path/to/your/dataset.xml"
    MIN_SAMPLES = 10

    print("Загрузка и предобработка данных...")
    processed_data = load_and_preprocess_xml(XML_PATH, min_class_samples=MIN_SAMPLES)

    print(f"Загружено {len(processed_data)} записей после фильтрации и предобработки.")
    if processed_
        print("Пример предобработанной записи:")
        print(processed_data[0])

        print("\n--- Статистика по датасету ---")
        num_classes = len(set(r['service'] for r in processed_data))
        print(f"Количество уникальных классов (после фильтрации): {num_classes}")

        service_counts_after_filter = Counter([r['service'] for r in processed_data])
        counts = list(service_counts_after_filter.values())

        print(f"Минимальное количество образцов в классе: {min(counts)}")
        print(f"Максимальное количество образцов в классе: {max(counts)}")
        print(f"Среднее количество образцов в классе: {np.mean(counts):.2f}")
        print(f"Медианное количество образцов в классе: {np.median(counts):.2f}")

        text_lengths = [len(r['combined_text']) for r in processed_data]
        print(f"Минимальная длина текста: {min(text_lengths)}")
        print(f"Максимальная длина текста: {max(text_lengths)}")
        print(f"Средняя длина текста: {np.mean(text_lengths):.2f}")

        # --- Визуализация ---
        fig, ax = plt.subplots(figsize=(14, 8))
        services = list(service_counts_after_filter.keys())
        counts_list = list(service_counts_after_filter.values())
        ax.bar(range(len(services)), counts_list)
        ax.set_title('Распределение количества образцов по классам (после фильтрации)')
        ax.set_xlabel('Класс (service)')
        ax.set_ylabel('Количество образцов')
        ax.set_xticks(range(len(services)))
        ax.set_xticklabels(services, rotation=90, ha="right") # Подписи под столбцами
        plt.tight_layout()
        plt.show()

        # --- Гистограмма длин текстов ---
        # Проверим, есть ли разнообразие в длинах
        unique_lengths = set(text_lengths)
        if len(unique_lengths) == 1:
             print(f"Все тексты имеют одинаковую длину: {text_lengths[0]} символов. Гистограмма неинформативна.")
        else:
            # Определим количество бинов разумно
            num_bins = min(50, len(unique_lengths))
            plt.figure(figsize=(10, 6))
            plt.hist(text_lengths, bins=num_bins, edgecolor='black')
            plt.title('Распределение длин текстов (combined_text)')
            plt.xlabel('Длина текста (символы)')
            plt.ylabel('Количество образцов')
            plt.show()
