TF-IDF + LogisticRegression как baseline
LightGBM с TF-IDF признаками

План:
Загрузка данных: Прочитаем processed_dataset.csv.
Разделение данных: Разделим на обучающую и тестовую выборки (train_test_split).
Векторизация (TF-IDF): Преобразуем тексты в числовые векторы с помощью TfidfVectorizer.
Обработка дисбаланса: Используем параметр class_weight='balanced' в LogisticRegression, который автоматически корректирует веса классов.
Подбор гиперпараметров: Используем GridSearchCV для поиска оптимальных параметров LogisticRegression (например, C, penalty).
Обучение: Обучим модель на обучающей выборке.
Предсказание: Сделаем предсказания на тестовой выборке.
Оценка: Выведем метрики (classification_report, confusion_matrix).
Анализ важных признаков: Найдем топ-слова для каждого класса на основе коэффициентов модели.
Визуализации: Построим матрицу ошибок, графики важности признаков (опционально).
Анализ дубликатов/похожих текстов: Обсудим, что можно сделать.

TF-IDF + LogisticRegression

Обоснование выбора TF-IDF + LogisticRegression:
Почему TF-IDF + LogisticRegression подходит для задачи классификации текстов:
Интерпретируемость: Это важное преимущество. Модель логистической регрессии присваивает вес (коэффициент) каждому признаку (в данном случае, каждому слову/биграмме после векторизации TF-IDF). Эти веса можно проанализировать, чтобы понять, какие слова наиболее характерны для каждого класса, как мы делали в выводе "Топ-5 важных слов". Это ценно для анализа и объяснения решений модели.
Эффективность: TF-IDF преобразует тексты в разреженные числовые векторы, которые логистическая регрессия обрабатывает быстро и эффективно, особенно по сравнению с более сложными моделями или нейронными сетями. Это особенно актуально для CPU.
Baseline: Это стандартный и надежный "базовый" метод (baseline) для задач классификации текстов. Он позволяет быстро получить представление о сложности задачи и качестве данных. Если даже простая модель показывает хороший результат, это хороший знак. Если результат плохой, возможно, нужно улучшать данные или использовать более сложные методы.
Учет важности слов: TF-IDF (Term Frequency-Inverse Document Frequency) оценивает важность слова в конкретном документе (тексте) относительно всего корпуса. Слова, которые часто встречаются в одном документе, но редко во всем корпусе, получают высокий вес. Это помогает модели сосредоточиться на характерных, отличительных словах для каждого класса.
Работа с дисбалансом: Использование параметра class_weight='balanced' позволяет модели учитывать дисбаланс классов, автоматически увеличивая "штраф" за ошибки на образцы меньших классов. Это улучшает способность модели распознавать и меньшие классы.
Простота настройки: По сравнению с деревьями решений или нейронными сетями, количество гиперпараметров у логистической регрессии относительно невелико (C, penalty, solver), что упрощает подбор.
Линейная разделимость: Логистическая регрессия строит линейные границы решения в пространстве признаков. TF-IDF создает пространство, где часто можно найти такие линейные зависимости между словами и классами. Это делает их хорошей парой для многих задач классификации текстов, особенно когда классы хотя бы частично линейно разделимы.
Как работает LogisticRegression и как выбирает класс:
Вход: Модель получает вектор признаков для текста, созданный TfidfVectorizer. Это вектор, где каждая позиция соответствует слову/биграмме, а значение — его TF-IDF весу в этом конкретном тексте.
Взвешивание: Модель имеет набор коэффициентов (coef_) — по одному для каждого признака (слова/биграммы) — для каждого класса. Также есть свободный член (intercept) для каждого класса.
Расчет "силы": Для каждого класса модель вычисляет линейную комбинацию признаков: score = (coef_1 * tfidf_value_1) + (coef_2 * tfidf_value_2) + ... + intercept. Это дает "оценку" или "силу" соответствия текста каждому классу.
Преобразование в вероятности: Эти оценки (scores) для всех классов преобразуются с помощью Softmax-функции, которая гарантирует, что сумма вероятностей всех классов будет равна 1. Получаются вероятности P(class_i | text).
Выбор класса: Модель выбирает класс с наивысшей вероятностью (или наивысшей оценкой score, если не использовать вероятности явно).
Обучение: Процесс обучения заключается в нахождении таких коэффициентов, которые максимизируют правдоподобие (или минимизируют ошибку, например, с кросс-энтропией), то есть так, чтобы модель как можно чаще правильно предсказывала метки обучающих примеров, с учетом регуляризации (параметр C) и балансировки классов (class_weight='balanced').
