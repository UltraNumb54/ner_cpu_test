# Убедитесь, что spaCy и ru_core_news_sm установлены
import sys
import spacy
from spacy.training import Example
import random
import json
import os
from pathlib import Path

# Загрузка предобученной модели
nlp = spacy.load("ru_core_news_sm")
print("Модель ru_core_news_sm успешно загружена.")

def load_data_from_json(json_path, text_base_dir):
    """
    Загружает данные из JSON-файла Label Studio и сопоставляет с текстовыми файлами.
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    examples = []
    for item in raw_data:
        # Извлекаем номер файла из пути в JSON
        json_file_path = item["text"]
        file_num_match = re.search(r'output_(\d+)\.txt$', json_file_path)
        if not file_num_match:
             print(f"Не удалось извлечь номер файла из пути: {json_file_path}")
             continue # Пропускаем, если не найден

        file_num = file_num_match.group(1)
        # Составляем путь к реальному файлу
        real_file_path = os.path.join(text_base_dir, f"output_{file_num}.txt")

        if not os.path.exists(real_file_path):
            print(f"Файл не найден: {real_file_path}")
            continue # Пропускаем, если файл не существует

        # Читаем текст из файла
        with open(real_file_path, 'r', encoding='utf-8') as txt_file:
            text = txt_file.read()

        # Подготовка аннотаций
        entities = []
        for label_info in item["label"]:
            start = label_info["start"]
            end = label_info["end"]
            label = label_info["labels"][0] # Предполагаем, что всегда одна метка

            # Проверка сдвига: выводим текст по указанным позициям
            # print(f"JSON Path: {json_file_path}, Start: {start}, End: {end}, Label: {label}")
            # print(f"Text at position [{start}:{end}]: '{text[start:end]}'")
            # print("---")

            entities.append((start, end, label))

        examples.append((text, {"entities": entities}))

    return examples

# --- ВАЖНО: Укажите пути к вашему датасету и папке с текстовыми файлами ---
import re # Импортируем регулярные выражения для поиска номера файла
json_dataset_path = "path/to/your/dataset.json" # ЗАМЕНИТЕ НА ПУТЬ К ВАШЕМУ JSON-ФАЙЛУ
text_files_directory = "path/to/your/text_files" # ЗАМЕНИТЕ НА ПУТЬ К ДИРЕКТОРИИ С TXT-ФАЙЛАМИ

# Загрузка данных
TRAIN_DATA = load_data_from_json(json_dataset_path, text_files_directory)
print(f"Загружено {len(TRAIN_DATA)} примеров для обучения.")

def train_spacy_model(train_data, n_iter=30):
    """
    Обучает модель spaCy NER на предоставленных данных.
    """
    # Получаем список уникальных меток из тренировочных данных
    labels = set()
    for _, annotations in train_data:
        for ent_start, ent_end, ent_label in annotations["entities"]:
            labels.add(ent_label)
    labels = list(labels)
    print(f"Обнаруженные метки: {labels}")

    # Проверяем, есть ли компонент NER, если нет - добавляем
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")

    # Добавляем ваши метки в компонент NER
    for label in labels:
        ner.add_label(label)

    # Отключаем другие компоненты для ускорения обучения
    pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

    # Начинаем обучение
    with nlp.disable_pipes(*unaffected_pipes):  # отключаем ненужные пайплайны
        optimizer = nlp.begin_training()
        for iteration in range(n_iter):
            print(f"Итерация {iteration + 1} из {n_iter}...")
            # Перемешиваем данные
            random.shuffle(train_data)
            losses = {}
            # Обновляем модель
            for text, annotations in train_data:
                 doc = nlp.make_doc(text)
                 example = Example.from_dict(doc, annotations)
                 nlp.update([example], losses=losses, drop=0.5) # drop - регуляризация
            print(f"Потери (losses): {losses}")

    return nlp

# Обучение модели (может занять несколько минут)
trained_nlp = train_spacy_model(TRAIN_DATA)
print("Обучение завершено.")

# Сохраните обученную модель в новую директорию
model_output_dir = "my_ner_model" # Имя директории для сохранения
trained_nlp.to_disk(model_output_dir)
print(f"Модель сохранена в: {model_output_dir}")

def test_model_on_new_text(model_path, input_text_path):
    """
    Загружает сохранённую модель и применяет её к новому текстовому файлу.
    """
    # Загрузка обученной модели
    nlp_test = spacy.load(model_path)

    # Чтение текста из указанного файла
    with open(input_text_path, 'r', encoding='utf-8') as f:
        text_to_test = f.read()

    print(f"Тестируемый текст:\n{text_to_test}\n")
    print("--- Результаты NER ---")

    # Применение модели
    doc = nlp_test(text_to_test)

    # Вывод найденных сущностей
    found_entities = []
    for ent in doc.ents:
        print(f"Текст: '{ent.text}' | Метка: {ent.label_} | Начало: {ent.start_char} | Конец: {ent.end_char}")
        found_entities.append((ent.text, ent.label_, ent.start_char, ent.end_char))

    if not found_entities:
        print("Сущностей не найдено.")

    return found_entities

# --- ВАЖНО: Укажите путь к сохранённой модели и к новому текстовому файлу ---
model_directory = "my_ner_model" # Путь к директории сохранённой модели
input_text_file_path = "path/to/your/test_input.txt" # ЗАМЕНИТЕ НА ПУТЬ К ВАШЕМУ ТЕСТОВОМУ TXT-ФАЙЛУ

# Запуск теста
entities_found = test_model_on_new_text(model_directory, input_text_file_path)
