print("   Токены в проблемной области:")
                for token in doc:
                    if token.idx >= start - 20 and token.idx <= end + 20:
                        print(f"      '{token.text}' (idx: {token.idx}, len: {len(token.text)})")
            else:
                print(f"✅ Span создан успешно: '{span.text}' -> {span.label_}")
                entities.append((start, end, label))

        examples.append((reconstructed_text, {"entities": entities}))


def prepare_training_data(spacy_nlp, train_tuples):
    """
    Преобразует список кортежей (text, annotations) в список Example для spaCy.
    """
    examples = []
    for text, annotations in train_tuples:
        # Создаем Doc из текста
        doc = spacy_nlp.make_doc(text)
        # Подготовим словарь с сущностями для spaCy
        ents = []
        for start, end, label in annotations["entities"]:
            # Проверяем, что start и end находятся в пределах текста
            if start < 0 or end > len(text):
                print(f"Предупреждение: сущность [{start}:{end}] вне диапазона текста длиной {len(text)}")
                continue
            # Получаем токен start и end, соответствующие сущности
            # spaCy работает с токенами, но мы можем задать span по символам
            span = doc.char_span(start, end, label=label)
            if span is not None:
                 ents.append(span)
            else:
                 # Если span не найден (например, из-за токенизации), пропускаем
                 print(f"Предупреждение: span [{start}:{end}] для метки '{label}' не найден в токенизированном документе.")
                 print(f"  Текст сущности: '{text[start:end]}'")
        # Присваиваем сущности документу
        doc.ents = ents
        # Создаем Example
        example = Example.from_dict(doc, annotations)
        examples.append(example)
    return examples

# --- ПУТИ К ВАШИМ ФАЙЛАМ ---
# ЗАМЕНИТЕ НА ВАШИ РЕАЛЬНЫЕ ПУТИ
json_dataset_path = "/path/to/your/actual/dataset.json" # Например, "./data/dataset.json"
text_files_directory = "/path/to/your/actual/text_files" # Например, "./data/texts/"
model_output_dir = "/path/to/save/your/trained/model" # Например, "./models/my_ner_model"

print(f"Попытка загрузки данных из: {json_dataset_path}")
print(f"Папка с текстовыми файлами: {text_files_directory}")

# Загрузка данных
TRAIN_DATA = load_data_from_json(json_dataset_path, text_files_directory)
print(f"Загружено {len(TRAIN_DATA)} примеров для обучения.")

if not TRAIN_DATA:
    print("Нет данных для обучения. Проверьте пути к файлам и формат данных.")
    exit()

# Подготовка данных для spaCy
print("Подготовка данных для spaCy...")
train_examples = prepare_training_data(nlp, TRAIN_DATA)
print(f"Подготовлено {len(train_examples)} примеров для spaCy.")

# --- НАСТРОЙКА И ОБУЧЕНИЕ МОДЕЛИ ---

# Проверяем, есть ли компонент 'ner', если нет - добавляем
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Добавляем ваши метки в компонент NER, если их там еще нет
labels = set()
for _, annotations in TRAIN_DATA:
    for _, _, label in annotations["entities"]:
        labels.add(label)

for label in labels:
    ner.add_label(label)

print(f"Добавлены метки: {list(labels)}")

# Настраиваем оптимизатор (может потребоваться настройка гиперпараметров)
optimizer = nlp.begin_training()
print("Начало обучения...")

# --- ЦИКЛ ОБУЧЕНИЯ ---
n_iter = 50 # Количество эпох обучения, настройте по необходимости
dropout = 0.35 # Вероятность дропаута, помогает избежать переобучения

for i in range(n_iter):
    print(f"Эпоха {i+1}/{n_iter}")
    random.shuffle(train_examples) # Перемешиваем данные перед каждой эпохой
    losses = {}
    for batch in spacy.util.minibatch(train_examples, size=2): # Размер батча можно настроить
        nlp.update(batch, drop=dropout, losses=losses, sgd=optimizer)
    print(f"  Потери: {losses}")

print("Обучение завершено.")

# --- СОХРАНЕНИЕ МОДЕЛИ ---
print(f"Сохранение модели в {model_output_dir}")
nlp.to_disk(model_output_dir)
print("Модель сохранена.")

# --- ТЕСТИРОВАНИЕ МОДЕЛИ ---
# Укажите путь к новому текстовому файлу для тестирования
test_file_path = "/path/to/your/test_file.txt" # ЗАМЕНИТЕ НА ВАШ ПУТЬ

if os.path.exists(test_file_path):
    print(f"Тестирование модели на файле: {test_file_path}")
    # Загрузим обученную модель из сохраненной директории
    trained_nlp = spacy.load(model_output_dir)

    with open(test_file_path, 'r', encoding='utf-8') as f:
        test_text = f.read()

    # Обработка текста
    doc = trained_nlp(test_text)

    print("\nНайденные сущности:")
    for ent in doc.ents:
        print(f"Текст: '{ent.text}', Метка: {ent.label_}, Начало: {ent.start_char}, Конец: {ent.end_char}")
else:
    print(f"Файл для тестирования не найден: {test_file_path}")
