
import sys
import spacy
from spacy.training import Example
from spacy.tokens import DocBin
import random
import json
import os
import re
from pathlib import Path

def simulate_label_studio_formatting(text: str) -> str:
    lines = text.splitlines()
    new_lines = []
    for i, line in enumerate(lines):
        new_lines.append(line)
        if i < len(lines) - 1:
            new_lines.append('')
    return '\n'.join(new_lines)

def load_data_from_json(json_path, text_base_dir):
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    examples = []
    for item in raw_data:
        json_file_path = item["text"]
        file_num_match = re.search(r'output_(\d+)\.txt$', json_file_path)
        if not file_num_match:
            continue

        file_num = file_num_match.group(1)
        real_file_path = os.path.join(text_base_dir, f"output_{file_num}.txt")

        if not os.path.exists(real_file_path):
            continue

        with open(real_file_path, 'r', encoding='utf-8') as txt_file:
            raw_text = txt_file.read()
            text = simulate_label_studio_formatting(raw_text)

        entities = []
        for label_info in item["label"]:
            start = int(label_info["start"])
            end = int(label_info["end"])
            label = label_info["labels"][0]
            if end <= len(text):
                entities.append((start, end, label))
        examples.append((text, {"entities": entities}))
    return examples

def train_spacy_model(train_data, n_iter=30):
    nlp = spacy.load("ru_core_news_sm")
    labels = set()
    for _, annotations in train_data:
        for ent_start, ent_end, ent_label in annotations["entities"]:
            labels.add(ent_label)
    labels = list(labels)

    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")

    for label in labels:
        ner.add_label(label)

    pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

    with nlp.disable_pipes(*unaffected_pipes):
        optimizer = nlp.begin_training()
        for iteration in range(n_iter):
            random.shuffle(train_data)
            losses = {}
            for text, annotations in train_data:
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                nlp.update([example], losses=losses, drop=0.5)
    return nlp

def test_model_on_new_text(model_path, input_text_path):
    nlp_test = spacy.load(model_path)
    with open(input_text_path, 'r', encoding='utf-8') as f:
        raw_text = f.read()
    text_to_test = simulate_label_studio_formatting(raw_text)
    doc = nlp_test(text_to_test)
    found_entities = []
    for ent in doc.ents:
        found_entities.append((ent.text, ent.label_, ent.start_char, ent.end_char))
    return found_entities

# === Пути (ЗАМЕНИТЕ НА СВОИ) ===
json_dataset_path = "/path/to/your/actual/dataset.json"
text_files_directory = "/path/to/your/actual/text_files"
input_text_file_path = "/path/to/your/test_input.txt"
model_output_dir = "my_ner_model"

# === Выполнение ===
TRAIN_DATA = load_data_from_json(json_dataset_path, text_files_directory)
trained_nlp = train_spacy_model(TRAIN_DATA)
trained_nlp.to_disk(model_output_dir)
entities_found = test_model_on_new_text(model_output_dir, input_text_file_path)

for ent in entities_found:
    print(f"Текст: '{ent[0]}' | Метка: {ent[1]} | Позиции: [{ent[2]}:{ent[3]}]")

///////////

def prepare_training_data(spacy_nlp, train_tuples):
    """
    Преобразует список кортежей (text, annotations) в список Example для spaCy.
    """
    examples = []
    for text, annotations in train_tuples:
        # Создаем Doc из текста
        doc = spacy_nlp.make_doc(text)
        # Подготовим словарь с сущностями для spaCy
        ents = []
        for start, end, label in annotations["entities"]:
            # Проверяем, что start и end находятся в пределах текста
            if start < 0 or end > len(text):
                print(f"Предупреждение: сущность [{start}:{end}] вне диапазона текста длиной {len(text)}")
                continue
            # Получаем токен start и end, соответствующие сущности
            # spaCy работает с токенами, но мы можем задать span по символам
            span = doc.char_span(start, end, label=label)
            if span is not None:
                 ents.append(span)
            else:
                 # Если span не найден (например, из-за токенизации), пропускаем
                 print(f"Предупреждение: span [{start}:{end}] для метки '{label}' не найден в токенизированном документе.")
                 print(f"  Текст сущности: '{text[start:end]}'")
        # Присваиваем сущности документу
        doc.ents = ents
        # Создаем Example
        example = Example.from_dict(doc, annotations)
        examples.append(example)
    return examples

# --- ПУТИ К ВАШИМ ФАЙЛАМ ---
# ЗАМЕНИТЕ НА ВАШИ РЕАЛЬНЫЕ ПУТИ
json_dataset_path = "/path/to/your/actual/dataset.json" # Например, "./data/dataset.json"
text_files_directory = "/path/to/your/actual/text_files" # Например, "./data/texts/"
model_output_dir = "/path/to/save/your/trained/model" # Например, "./models/my_ner_model"

print(f"Попытка загрузки данных из: {json_dataset_path}")
print(f"Папка с текстовыми файлами: {text_files_directory}")

# Загрузка данных
TRAIN_DATA = load_data_from_json(json_dataset_path, text_files_directory)
print(f"Загружено {len(TRAIN_DATA)} примеров для обучения.")

if not TRAIN_DATA:
    print("Нет данных для обучения. Проверьте пути к файлам и формат данных.")
    exit()

# Подготовка данных для spaCy
print("Подготовка данных для spaCy...")
train_examples = prepare_training_data(nlp, TRAIN_DATA)
print(f"Подготовлено {len(train_examples)} примеров для spaCy.")

# --- НАСТРОЙКА И ОБУЧЕНИЕ МОДЕЛИ ---

# Проверяем, есть ли компонент 'ner', если нет - добавляем
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Добавляем ваши метки в компонент NER, если их там еще нет
labels = set()
for _, annotations in TRAIN_DATA:
    for _, _, label in annotations["entities"]:
        labels.add(label)

for label in labels:
    ner.add_label(label)

print(f"Добавлены метки: {list(labels)}")

# Настраиваем оптимизатор (может потребоваться настройка гиперпараметров)
optimizer = nlp.begin_training()
print("Начало обучения...")

# --- ЦИКЛ ОБУЧЕНИЯ ---
n_iter = 50 # Количество эпох обучения, настройте по необходимости
dropout = 0.35 # Вероятность дропаута, помогает избежать переобучения

for i in range(n_iter):
    print(f"Эпоха {i+1}/{n_iter}")
    random.shuffle(train_examples) # Перемешиваем данные перед каждой эпохой
    losses = {}
    for batch in spacy.util.minibatch(train_examples, size=2): # Размер батча можно настроить
        nlp.update(batch, drop=dropout, losses=losses, sgd=optimizer)
    print(f"  Потери: {losses}")

print("Обучение завершено.")

# --- СОХРАНЕНИЕ МОДЕЛИ ---
print(f"Сохранение модели в {model_output_dir}")
nlp.to_disk(model_output_dir)
print("Модель сохранена.")

# --- ТЕСТИРОВАНИЕ МОДЕЛИ ---
# Укажите путь к новому текстовому файлу для тестирования
test_file_path = "/path/to/your/test_file.txt" # ЗАМЕНИТЕ НА ВАШ ПУТЬ

if os.path.exists(test_file_path):
    print(f"Тестирование модели на файле: {test_file_path}")
    # Загрузим обученную модель из сохраненной директории
    trained_nlp = spacy.load(model_output_dir)

    with open(test_file_path, 'r', encoding='utf-8') as f:
        test_text = f.read()

    # Обработка текста
    doc = trained_nlp(test_text)

    print("\nНайденные сущности:")
    for ent in doc.ents:
        print(f"Текст: '{ent.text}', Метка: {ent.label_}, Начало: {ent.start_char}, Конец: {ent.end_char}")
else:
    print(f"Файл для тестирования не найден: {test_file_path}")
