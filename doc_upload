def normalize_whitespace(text):
    """Заменяет любую последовательность пробельных символов (включая \n, \t) на один пробел."""
    return re.sub(r'\s+', ' ', text)

def load_data_from_json(json_path, text_base_dir):
    """
    Загружает данные из JSON-файла Label Studio и сопоставляет с текстовыми файлами,
    нормализуя пробельные символы для сопоставления координат сущностей.
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    examples = []
    for item in raw_data:
        # Извлекаем номер файла из пути в JSON
        json_file_path = item["text"]
        file_num_match = re.search(r'output_(\d+)\.txt$', json_file_path)
        if not file_num_match:
             print(f"Не удалось извлечь номер файла из пути: {json_file_path}")
             continue # Пропускаем, если не найден

        file_num = file_num_match.group(1)
        # Составляем путь к реальному файлу
        real_file_path = os.path.join(text_base_dir, f"output_{file_num}.txt")

        if not os.path.exists(real_file_path):
            print(f"Файл не найден: {real_file_path}")
            continue # Пропускаем, если файл не существует

        # Читаем текст из файла
        with open(real_file_path, 'r', encoding='utf-8') as txt_file:
            text = txt_file.read()

        # Подготовка аннотаций
        entities = []
        for label_info in item["label"]:
            original_start = int(label_info["start"])
            original_end = int(label_info["end"])
            label = label_info["labels"][0]

            # Извлекаем текст, который Label Studio считала сущностью (по оригинальным координатам)
            try:
                original_span = text[original_start:original_end]
            except IndexError:
                print(f"ПРЕДУПРЕЖДЕНИЕ: Оригинальные индексы [{original_start}:{original_end}] выходят за границы текста файла {real_file_path}. Пропускаем сущность.")
                continue

            # Нормализуем пробелы в сущности из JSON
            normalized_original_span = normalize_whitespace(original_span)

            # Нормализуем весь текст файла
            normalized_text = normalize_whitespace(text)

            # Ищем нормализованную сущность в нормализованном тексте
            normalized_pos = normalized_text.find(normalized_original_span)

            if normalized_pos != -1:
                escaped_span = re.escape(original_span)
                # Заменяем любую последовательность пробельных символов на \s+
                regex_pattern = re.sub(r'\s+', r'\\s+', escaped_span)

                match = re.search(regex_pattern, text)
                if match:
                    corrected_start = match.start()
                    corrected_end = match.end()
                    matched_text = text[corrected_start:corrected_end]
                    print(f"Сущность (с гибким поиском) '{original_span}' (метка: {label}) найдена в файле {real_file_path} по координатам [{corrected_start}:{corrected_end}]. Найденный текст: '{matched_text}'")
                else:
                    print(f"ПРЕДУПРЕЖДЕНИЕ: Сущность '{original_span}' (метка: {label}) не найдена в файле {real_file_path} даже с гибким поиском по пробельным символам. Пропускаем сущность.")
                    continue
            else:
                # Даже нормализованный поиск не помог
                 print(f"ПРЕДУПРЕЖДЕНИЕ: Нормализованная сущность '{normalized_original_span}' не найдена в нормализованном тексте файла {real_file_path}. Пропускаем сущность.")
                 continue

            # --- /Попытка сопоставления ---

            # Проверим, не выходит ли сдвиг за границы текста (на всякий случай)
            if corrected_start < 0 or corrected_end > len(text) or corrected_start > corrected_end:
                print(f"ПРЕДУПРЕЖДЕНИЕ: Сопоставленные индексы [{corrected_start}:{corrected_end}] некорректны для файла {real_file_path}. Пропускаем сущность.")
                continue

            # Проверка: выводим текст по сопоставленным позициям
            final_matched_text = text[corrected_start:corrected_end]
            print(f"ИТОГО: для метки '{label}' в файле {real_file_path}, текст: '{final_matched_text}', координаты: [{corrected_start}:{corrected_end}]")
            print("---")

            entities.append((corrected_start, corrected_end, label))

        examples.append((text, {"entities": entities}))

    return examples
///////////////////////

import sys
import spacy
from spacy.training import Example
import random
import json
import os
from pathlib import Path
import re

# Загрузка предобученной модели
nlp = spacy.load("ru_core_news_sm")
print("Модель ru_core_news_sm успешно загружена.")

def load_data_from_json(json_path, text_base_dir):
    """
    Загружает данные из JSON-файла Label Studio и сопоставляет с текстовыми файлами.
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    examples = []
    for item in raw_data:
        # Извлекаем номер файла из пути в JSON
        json_file_path = item["text"]
        file_num_match = re.search(r'output_(\d+)\.txt$', json_file_path)
        if not file_num_match:
             print(f"Не удалось извлечь номер файла из пути: {json_file_path}")
             continue

        file_num = file_num_match.group(1)
 
        real_file_path = os.path.join(text_base_dir, f"output_{file_num}.txt")

        if not os.path.exists(real_file_path):
            print(f"Файл не найден: {real_file_path}")
            continue # Пропускаем, если файл не существует

        # Читаем текст из файла
        with open(real_file_path, 'r', encoding='utf-8') as txt_file:
            text = txt_file.read()

        # Подготовка аннотаций
        entities = []
        for label_info in item["label"]:
         
            start = int(label_info["start"])
            end = int(label_info["end"])
            label = label_info["labels"][0]

     
            print(f"JSON Path: {json_file_path}, Real Path: {real_file_path}")
            print(f"Start: {start}, End: {end}, Label: {label}")
            print(f"Text at position [{start}:{end}]: '{text[start:end]}'")
            print("---")

            entities.append((start, end, label))

        examples.append((text, {"entities": entities}))

    return examples


json_dataset_path = "/path/to/your/actual/dataset.json"
text_files_directory = "/path/to/your/actual/text_files"

print(f"Попытка загрузки данных из: {json_dataset_path}")
print(f"Папка с текстовыми файлами: {text_files_directory}")

# Загрузка данных
TRAIN_DATA = load_data_from_json(json_dataset_path, text_files_directory)
print(f"Загружено {len(TRAIN_DATA)} примеров для обучения.")

print(f"Тип TRAIN_DATA: {type(TRAIN_DATA)}")
if TRAIN_DATA:
    first_item = TRAIN_DATA[0]
    print(f"Тип первого элемента TRAIN_DATA: {type(first_item)}")
    print(f"Пример содержимого первого элемента (текст[:100]): {first_item[0][:100]}...")

def train_spacy_model(train_data, n_iter=30):
    """
    Обучает модель spaCy NER на предоставленных данных.
    """
    # Получаем список уникальных меток из тренировочных данных
    labels = set()
    for _, annotations in train_data:
        for ent_start, ent_end, ent_label in annotations["entities"]:
            labels.add(ent_label)
    labels = list(labels)
    print(f"Обнаруженные метки: {labels}")

    # Проверяем, есть ли компонент NER, если нет - добавляем
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")

    # Добавляем ваши метки в компонент NER
    for label in labels:
        ner.add_label(label)

    # Отключаем другие компоненты для ускорения обучения
    pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

    # Начинаем обучение
    with nlp.disable_pipes(*unaffected_pipes):  # отключаем ненужные пайплайны
        optimizer = nlp.begin_training()
        for iteration in range(n_iter):
            print(f"Итерация {iteration + 1} из {n_iter}...")
            # Перемешиваем данные
            random.shuffle(train_data)
            losses = {}
            # Обновляем модель
            for text, annotations in train_data:
                 doc = nlp.make_doc(text)
                 example = Example.from_dict(doc, annotations)
                 nlp.update([example], losses=losses, drop=0.5) # drop - регуляризация
            print(f"Потери (losses): {losses}")

    return nlp

# Обучение модели (может занять несколько минут)
trained_nlp = train_spacy_model(TRAIN_DATA)
print("Обучение завершено.")

# Сохраните обученную модель в новую директорию
model_output_dir = "my_ner_model" # Имя директории для сохранения
trained_nlp.to_disk(model_output_dir)
print(f"Модель сохранена в: {model_output_dir}")

def test_model_on_new_text(model_path, input_text_path):
    """
    Загружает сохранённую модель и применяет её к новому текстовому файлу.
    """
    # Загрузка обученной модели
    nlp_test = spacy.load(model_path)

    # Чтение текста из указанного файла
    with open(input_text_path, 'r', encoding='utf-8') as f:
        text_to_test = f.read()

    print(f"Тестируемый текст:\n{text_to_test}\n")
    print("--- Результаты NER ---")

    # Применение модели
    doc = nlp_test(text_to_test)

    # Вывод найденных сущностей
    found_entities = []
    for ent in doc.ents:
        print(f"Текст: '{ent.text}' | Метка: {ent.label_} | Начало: {ent.start_char} | Конец: {ent.end_char}")
        found_entities.append((ent.text, ent.label_, ent.start_char, ent.end_char))

    if not found_entities:
        print("Сущностей не найдено.")

    return found_entities

# --- ВАЖНО: Укажите путь к сохранённой модели и к новому текстовому файлу ---
model_directory = "my_ner_model" # Путь к директории сохранённой модели
input_text_file_path = "path/to/your/test_input.txt" # ЗАМЕНИТЕ НА ПУТЬ К ВАШЕМУ ТЕСТОВОМУ TXT-ФАЙЛУ

# Запуск теста
entities_found = test_model_on_new_text(model_directory, input_text_file_path)
